{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split tables to many markdown files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the many markdown files\n",
    "import os\n",
    "\n",
    "# Define the path to the uploaded file\n",
    "input_file_path = 'tables.md'\n",
    "\n",
    "# Define a directory to save the split markdown files\n",
    "output_dir = 'split_markdown_files'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Initialize variables\n",
    "current_header = None\n",
    "current_content = []\n",
    "\n",
    "# Read the file\n",
    "with open(input_file_path, 'r') as file:\n",
    "    lines = file.readlines()\n",
    "\n",
    "# Process the file line by line\n",
    "for line in lines:\n",
    "    # Check if the line is a header (##)\n",
    "    if line.startswith('## '):\n",
    "        # If there is current content, save it to a new file\n",
    "        if current_header and current_content:\n",
    "            output_file_path = os.path.join(output_dir, f\"{current_header.strip()}.md\")\n",
    "            with open(output_file_path, 'w') as output_file:\n",
    "                output_file.writelines(current_content)\n",
    "        \n",
    "        # Start a new file\n",
    "        current_header = line[3:].strip()  # Remove '## ' from the header\n",
    "        current_content = [line]  # Start the content with the current header line\n",
    "    \n",
    "    elif line.strip() == '--- END ---':\n",
    "        # Add the end marker to the content\n",
    "        current_content.append(line)\n",
    "        \n",
    "        # Save the current content to a file\n",
    "        if current_header:\n",
    "            output_file_path = os.path.join(output_dir, f\"{current_header}.md\")\n",
    "            with open(output_file_path, 'w') as output_file:\n",
    "                output_file.writelines(current_content)\n",
    "        \n",
    "        # Reset for the next section\n",
    "        current_header = None\n",
    "        current_content = []\n",
    "    \n",
    "    else:\n",
    "        # Continue collecting lines for the current section\n",
    "        current_content.append(line)\n",
    "\n",
    "# Handle the last section if it wasn't closed by '--- END ---'\n",
    "if current_header and current_content:\n",
    "    output_file_path = os.path.join(output_dir, f\"{current_header}.md\")\n",
    "    with open(output_file_path, 'w') as output_file:\n",
    "        output_file.writelines(current_content)\n",
    "\n",
    "print(f\"Markdown files have been split and saved to {output_dir}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Replace info from the tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace the info need from the markdown files\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "\n",
    "def extract_section(md_content, section_name):\n",
    "    \"\"\"\n",
    "    Extracts a section from the markdown content.\n",
    "    \"\"\"\n",
    "    pattern = re.compile(rf\"### {section_name}(.*?)## \", re.S)\n",
    "    match = pattern.search(md_content)\n",
    "    if match:\n",
    "        return match.group(1).strip()\n",
    "    return None\n",
    "\n",
    "def remove_first_and_last_line(file_path):\n",
    "    # Open and read the file content\n",
    "    with open(file_path, 'r') as file:\n",
    "        lines = file.readlines()\n",
    "\n",
    "    # Remove the first and last lines\n",
    "    if len(lines) > 3:\n",
    "        lines = lines[2:-1]\n",
    "    else:\n",
    "        lines = []  # In case the file has only one or two lines\n",
    "\n",
    "    # Join the remaining lines\n",
    "    final_content = ''.join(lines)\n",
    "\n",
    "    return final_content\n",
    "\n",
    "table_sections = [\n",
    "    \"Participant Flow Table\",\n",
    "    \"Baseline Characteristics\",\n",
    "    \"Primary Outcome Result(s)\",\n",
    "    \"Secondary Outcome Result(s)\",\n",
    "    \"All-Cause Mortality\",\n",
    "    \"Serious Adverse Events\",\n",
    "    \"Other .* Adverse Events\",\n",
    "    \"Safety Results\"\n",
    "]\n",
    "\n",
    "# Load the JSON file\n",
    "with open('database.json', 'r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# Iterate through each trial\n",
    "for trial in data:\n",
    "    trial_name = trial['trial_name']\n",
    "    md_file_path = f\"./split_markdown_files/{trial_name}.md\"\n",
    "    \n",
    "    # Check if the markdown file exists\n",
    "    if os.path.exists(md_file_path):\n",
    "        # print(md_file_path)\n",
    "        md_content = remove_first_and_last_line(md_file_path)\n",
    "\n",
    "        for section in table_sections:\n",
    "            if section in trial['trial'].keys():\n",
    "                del trial['trial'][section]\n",
    "\n",
    "            # Update the trial information in JSON\n",
    "            trial['trial']['Trial Analysis'] = md_content\n",
    "\n",
    "# Save the updated JSON file\n",
    "with open('database.json', 'w') as file:\n",
    "    json.dump(data, file, indent=4)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean up Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spellchecker import SpellChecker\n",
    "\n",
    "# Initialize the spell checker\n",
    "spell = SpellChecker()\n",
    "\n",
    "def is_valid_word(word):\n",
    "    return word in spell\n",
    "\n",
    "def fix_united_words(text):\n",
    "    words = []\n",
    "    start = 0\n",
    "    while start < len(text):\n",
    "        for end in range(len(text), start, -1):\n",
    "            candidate = text[start:end]\n",
    "            if is_valid_word(candidate):\n",
    "                words.append(candidate)\n",
    "                start = end\n",
    "                break\n",
    "        else:\n",
    "            words.append(text[start])\n",
    "            start += 1\n",
    "    return words\n",
    "\n",
    "def combine_single_characters(words):\n",
    "    combined_words = []\n",
    "    temp = []\n",
    "    \n",
    "    for word in words:\n",
    "        if len(word) == 1:\n",
    "            temp.append(word)\n",
    "        else:\n",
    "            if len(temp) > 0:\n",
    "                combined_words.append(''.join(temp))\n",
    "                temp = []\n",
    "            combined_words.append(word)\n",
    "    \n",
    "    # Add any remaining single characters as a combined word\n",
    "    if len(temp) > 0:\n",
    "        combined_words.append(''.join(temp))\n",
    "    \n",
    "    return combined_words\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Step 1: Fix united words\n",
    "    words = fix_united_words(text)\n",
    "    \n",
    "    # Step 2: Combine single characters\n",
    "    combined_words = combine_single_characters(words)\n",
    "    \n",
    "    # Join the final list of words into a string\n",
    "    output = ' '.join(combined_words)\n",
    "    return ' '.join(output.split())\n",
    "\n",
    "# Example usage\n",
    "text = \"This trial helped learn about the safety of different doses of LHC165 given alone or with PDR001 inparticipants with advanced cancers. The researchers concluded that 600 g LHC165 was the highestdose that was safe for participants to receive alone or with 400 mg PDR001.Because enrollment ended early and there were too few participants, the researchers could not makeany conclusions about the effects of LHC165 given alone or with PDR001 on shrinking cancer. Thesponsor has no plans for other trials of LHC165 in people with advanced cancers.\"\n",
    "fixed_text = preprocess_text(text)\n",
    "print(fixed_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "file_path = 'database.json'\n",
    "# file_path = 'small_database.json'\n",
    "with open(file_path, 'r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# Define the terms to be replaced with space\n",
    "terms_to_replace = [\n",
    "    \"www.novartisclinicaltrials.com\", \"1-888-669-6682 (US);\", \"1-888-669-6682 (US)\", \"1-888-669-6682  (US)\",\n",
    "    \"+41-61-324-1111 (EU);\", \"www.novctrd.com\", \"www.clinicaltrials.gov\", \"+41-61-324-1111 (EU)\",\n",
    "    \"https://www.clinicaltrialsregister.eu/ctr-search/search\",\n",
    "    \"Clinical Trial Results Website\", \"+41-61-324 1111 (EU);\", \"+41-61-324 1111 (EU)\",\n",
    "    \"www.clinicaltrialsregister.eu\", \"http://www.novartis.com/clinicaltrials\",\n",
    "    \"| Adults and Adolescent version |Trial Results Summary | 18\", \"| Adults and Adolescent version |Trial Results Summary | 17\", \"| Adults and Adolescent version |Trial Results Summary | 16\", \"| Adults and Adolescent version |Trial Results Summary | 15\",\n",
    "    \"| Adults and Adolescent version |Trial Results Summary | 14\", \"| Adults and Adolescent version |Trial Results Summary | 13\", \"| Adults and Adolescent version |Trial Results Summary | 12\", \"| Adults and Adolescent version |Trial Results Summary | 11\",\n",
    "    \"| Adults and Adolescent version |Trial Results Summary | 10\", \"| Adults and Adolescent version |Trial Results Summary | 9\", \"| Adults and Adolescent version |Trial Results Summary | 8\", \"| Adults and Adolescent version |Trial Results Summary | 7\",\n",
    "    \"| Adults and Adolescent version |Trial Results Summary | 6\", \"| Adults and Adolescent version |Trial Results Summary | 5\", \"| Adults and Adolescent version |Trial Results Summary | 4\", \"| Adults and Adolescent version |Trial Results Summary | 3\",\n",
    "    \"| Adults and Adolescent version |Trial Results Summary | 2\", \"| Adults and Adolescent version |Trial Results Summary | 1\",\n",
    "    \"| Trial Results Summary | 18\", \"| Trial Results Summary | 17\", \"| Trial Results Summary | 16\", \"| Trial Results Summary | 15\", \"| Trial Results Summary | 14\", \"| Trial Results Summary | 13\", \"| Trial Results Summary | 12\", \"| Trial Results Summary | 11\",\n",
    "    \"| Trial Results Summary | 10\", \"| Trial Results Summary | 9\", \"| Trial Results Summary | 8\", \"| Trial Results Summary | 7\", \"| Trial Results Summary | 6\", \"| Trial Results Summary | 5\", \"| Trial Results Summary | 4\", \"| Trial Results Summary | 3\",\n",
    "    \"| Trial Results Summary | 2\", \"| Trial Results Summary | 1\",\n",
    "    \"| Trial Results Summary | Parent Version | 18\", \"| Trial Results Summary | Parent Version | 17\", \"| Trial Results Summary | Parent Version | 16\", \"| Trial Results Summary | Parent Version | 15\",\n",
    "    \"| Trial Results Summary | Parent Version | 14\", \"| Trial Results Summary | Parent Version | 13\", \"| Trial Results Summary | Parent Version | 12\", \"| Trial Results Summary | Parent Version | 11\",\n",
    "    \"| Trial Results Summary | Parent Version | 10\", \"| Trial Results Summary | Parent Version | 9\", \"| Trial Results Summary | Parent Version | 8\", \"| Trial Results Summary | Parent Version | 7\",\n",
    "    \"| Trial Results Summary | Parent Version | 6\", \"| Trial Results Summary | Parent Version | 5\", \"| Trial Results Summary | Parent Version | 4\", \"| Trial Results Summary | Parent Version | 3\",\n",
    "    \"| Trial Results Summary | Parent Version | 2\", \"| Trial Results Summary | Parent Version | 1\", \"| Trial Results Summary | Adult Version | 18\", \"| Trial Results Summary | Adult Version | 17\",\n",
    "    \"| Trial Results Summary | Adult Version | 16\", \"| Trial Results Summary | Adult Version | 15\", \"| Trial Results Summary | Adult Version | 14\", \"| Trial Results Summary | Adult Version | 13\",\n",
    "    \"| Trial Results Summary | Adult Version | 12\", \"| Trial Results Summary | Adult Version | 11\", \"| Trial Results Summary | Adult Version | 10\", \"| Trial Results Summary | Adult Version | 9\",\n",
    "    \"| Trial Results Summary | Adult Version | 8\", \"| Trial Results Summary | Adult Version | 7\", \"| Trial Results Summary | Adult Version | 6\", \"| Trial Results Summary | Adult Version | 5\",\n",
    "    \"| Trial Results Summary | Adult Version | 4\", \"| Trial Results Summary | Adult Version | 3\", \"| Trial Results Summary | Adult Version | 2\", \"| Trial Results Summary | Adult Version | 1\"\n",
    "]\n",
    "\n",
    "# Function to replace the terms and trial names with space\n",
    "def replace_terms(text, trial_name, debug=False):\n",
    "    for term in terms_to_replace:\n",
    "        text = text.replace(term, ' ')\n",
    "    text = text.replace(trial_name, ' ')\n",
    "    return text\n",
    "\n",
    "# Process each trial in the JSON data\n",
    "for trial in data:\n",
    "    trial_name = trial['trial_name']\n",
    "    print(trial_name)\n",
    "    \n",
    "    # Clean up extra spaces\n",
    "    for key in trial['trial']:\n",
    "        if isinstance(trial['trial'][key], str):\n",
    "            trial['trial'][key] = ' '.join(trial['trial'][key].split())\n",
    "    \n",
    "    for key in trial['summary']:\n",
    "        if isinstance(trial['summary'][key], str):\n",
    "            trial['summary'][key] = ' '.join(trial['summary'][key].split())\n",
    "\n",
    "    # Replace in trial sections\n",
    "    for key in trial['trial']:\n",
    "        if isinstance(trial['trial'][key], str):\n",
    "            trial['trial'][key] = replace_terms(trial['trial'][key], trial_name)\n",
    "            if(key!=\"Trial Analysis\" and key!=\"Generic Drug Name\" and key!=\"Protocol Number\" and key!=\"Date of Clinical Trial Report\"):\n",
    "                trial['trial'][key] = preprocess_text(trial['trial'][key])\n",
    "    \n",
    "    # Replace in summary sections\n",
    "    for key in trial['summary']:\n",
    "        if isinstance(trial['summary'][key], str):\n",
    "            trial['summary'][key] = replace_terms(trial['summary'][key], trial_name, True)\n",
    "            if(key!=\"Thank you\"):\n",
    "                trial['summary'][key] = preprocess_text(trial['summary'][key])\n",
    "\n",
    "    # Clean up extra spaces\n",
    "    for key in trial['trial']:\n",
    "        if isinstance(trial['trial'][key], str):\n",
    "            trial['trial'][key] = ' '.join(trial['trial'][key].split())\n",
    "    \n",
    "    for key in trial['summary']:\n",
    "        if isinstance(trial['summary'][key], str):\n",
    "            trial['summary'][key] = ' '.join(trial['summary'][key].split())\n",
    "\n",
    "# Save the modified data to a new JSON file\n",
    "output_file_path = 'database_clean2.json'\n",
    "with open(output_file_path, 'w') as file:\n",
    "    json.dump(data, file, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import json\n",
    "\n",
    "# # Load the JSON data\n",
    "# file_path = 'updated_database.json'\n",
    "# with open(file_path, 'r') as file:\n",
    "#     data = json.load(file)\n",
    "\n",
    "# # Create a directory to store the output files\n",
    "# output_directory = 'TrialsOutput'\n",
    "# if not os.path.exists(output_directory):\n",
    "#     os.makedirs(output_directory)\n",
    "\n",
    "# # Iterate over each entry in the JSON data\n",
    "# for entry in data:\n",
    "#     trial_name = entry['trial_name']\n",
    "    \n",
    "#     # Create file names\n",
    "#     trial_file_name = f\"{trial_name}_trial.txt\"\n",
    "#     summary_file_name = f\"{trial_name}_summary.txt\"\n",
    "    \n",
    "#     # File paths\n",
    "#     trial_file_path = os.path.join(output_directory, trial_file_name)\n",
    "#     summary_file_path = os.path.join(output_directory, summary_file_name)\n",
    "    \n",
    "#     # Write the trial data to the trial file\n",
    "#     with open(trial_file_path, 'w') as trial_file:\n",
    "#         trial_content = json.dumps(entry['trial'], indent=4)\n",
    "#         trial_file.write(trial_content)\n",
    "    \n",
    "#     # Write the summary data to the summary file\n",
    "#     with open(summary_file_path, 'w') as summary_file:\n",
    "#         summary_content = json.dumps(entry['summary'], indent=4)\n",
    "#         summary_file.write(summary_content)\n",
    "\n",
    "# print(f\"Files have been created in the {output_directory} directory.\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Database Mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from unicodedata import normalize\n",
    "\n",
    "# Load the JSON data\n",
    "with open('database_clean2.json') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "mapping = {\n",
    "   \"Why was the research needed?\": [\"Objectives\"],\n",
    "   \"How long was the trial?\": [\"Study Start/End Dates\", \"Reason for Termination\"],\n",
    "   \"Who was in this clinical trial?\": [\"Study Population: Key Inclusion/Exclusion Criteria\"],\n",
    "   \"What treatments did the participants take?\": [\"Statistical Methods\", \"Trial Analysis\"],\n",
    "   \"What happened during the trial?\": [\"Study Design/Methodology\"],\n",
    "   \"What were the results of the trial?\": [\"Trial Analysis\", \"Conclusion\"],\n",
    "   \"What adverse events did participants report?\": [\"Trial Analysis\"],\n",
    "   \"How has this trial helped?\": [\"Conclusion\"],\n",
    "}\n",
    "\n",
    "# Iterate over each key in the mapping\n",
    "for question, sections in mapping.items():\n",
    "    print(question)\n",
    "    output = []\n",
    "    \n",
    "    for trial in data:\n",
    "        trial_name = trial['trial_name']\n",
    "        print(trial_name)\n",
    "        context = \"\"\n",
    "        answer = \"\"\n",
    "\n",
    "        for section in sections:\n",
    "            # Fetch the context from the trial data\n",
    "            if section in trial['trial']:\n",
    "                context += trial['trial'][section] + \"\\n\"\n",
    "            \n",
    "        # Fetch the answer from the summary data\n",
    "        if question in trial['summary']:\n",
    "            answer += trial['summary'][question]\n",
    "        \n",
    "        # Create the entry for this trial\n",
    "        context = normalize('NFKD', context.strip()).encode('ascii','ignore')\n",
    "        answer = normalize('NFKD', answer.strip()).encode('ascii','ignore')\n",
    "        context = context.decode(encoding=\"utf-8\")\n",
    "        answer = answer.decode(encoding=\"utf-8\")\n",
    "        if answer and context:\n",
    "            output.append({\n",
    "                \"trial_name\": trial_name,\n",
    "                \"question\": question,\n",
    "                \"context\": ' '.join(context.split()),\n",
    "                \"answer\": ' '.join(answer.split())\n",
    "            })\n",
    "    \n",
    "    # Write the output to a JSON file\n",
    "    with open(f'FinalDataset/FullDataset2/{question.replace(\" \", \"_\").replace(\"?\", \"\")}.json', 'w') as outfile:\n",
    "        json.dump(output, outfile, indent=4)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split train and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "import os\n",
    "\n",
    "# Define the folder containing the JSON files\n",
    "folder_path = 'FinalDataset/FullDataset2/' \n",
    "folder_path2 = 'FinalDataset/'\n",
    "\n",
    "# Get a list of all JSON files in the folder\n",
    "json_files = [f for f in os.listdir(folder_path) if f.endswith('.json')]\n",
    "\n",
    "# Process each JSON file\n",
    "for json_file in json_files:\n",
    "    file_path = os.path.join(folder_path, json_file)\n",
    "    \n",
    "    # Load the JSON file\n",
    "    with open(file_path, 'r') as file:\n",
    "        data = json.load(file)\n",
    "    \n",
    "    # Shuffle the data randomly\n",
    "    random.shuffle(data)\n",
    "    \n",
    "    # Calculate the split index\n",
    "    split_index = int(0.85 * len(data))\n",
    "    \n",
    "    # Split the data into training and testing sets\n",
    "    train_data = data[:split_index]\n",
    "    test_data = data[split_index:]\n",
    "    \n",
    "    # Create filenames for the split datasets\n",
    "    train_file_path = os.path.join(folder_path2, f'train_{json_file}')\n",
    "    test_file_path = os.path.join(folder_path2, f'test_{json_file}')\n",
    "    \n",
    "    # Save the split datasets into separate JSON files\n",
    "    with open(train_file_path, 'w') as train_file:\n",
    "        json.dump(train_data, train_file, indent=4)\n",
    "    \n",
    "    with open(test_file_path, 'w') as test_file:\n",
    "        json.dump(test_data, test_file, indent=4)\n",
    "    \n",
    "    print(f\"Processed and split {json_file} into training and testing sets.\")\n",
    "\n",
    "print(\"All files have been successfully processed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
